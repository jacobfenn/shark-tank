<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Digital humanities research project exploring [your topic]">
    <meta name="author" content="Your Name">
    <title>Your Project Title | WRIT 20833</title>
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <nav>
        <ul>
            <li><a href="#question">Research Question</a></li>
            <li><a href="#data">Data & Methods</a></li>
            <li><a href="#analysis">Results & Analysis</a></li>
            <li><a href="#findings">Findings</a></li>
            <li><a href="#reflection">Reflection</a></li>
        </ul>
    </nav>

    <header>
        <h1>Shark Tank Around The World</h1>
        <p>Mia Eberle, Jacob Fenn, Zane Lemaster | WRIT 20833 | Fall 2025</p>
    </header>

    <main>
        <section id="question">
            <h2>Research Question</h2>
            <How has the perception of the sharks on Shark Tank changed over time, along with the general perception of the entire show, as cultures and the world have evolved?
>
            <p>How has the perception of the sharks on Shark Tank changed over time, along with the general perception of the entire show, as cultures and the world have evolved?</p>

            <p><strong>Background:</strong> This research question matters because Shark Tank has been around for so long that people‚Äôs opinions about the show and the sharks have definitely changed, especially as the internet has grown. The sharks are celebrities now, and the way people talk about them online can shape how the show is viewed as a whole. Since online culture has become louder and more opinionated over the years, we wanted to see if those shifts actually show up in the comments and reviews or if the reality is different from what we assume.
We were motivated to study this because earlier in the class we saw how our expectations about online negativity didn‚Äôt always match what the data showed. That made us curious about what might happen if we looked at something bigger and more well-known, like Shark Tank, over a long period of time. By comparing comments from older seasons to newer ones, we want to understand how people‚Äôs reactions have changed and why. Overall, this question matters because it helps us see how TV, internet culture, and public opinion all influence each other, and how using both data tools and human interpretation can reveal things we wouldn‚Äôt notice on our own.</p>
        </section>

        <section id="data">
            <h2>Data & Methods</h2>

            <h3>Dataset</h3>
            <!-- TODO: Describe your data collection and methodology -->
            <p><strong>Data Source:</strong> Our data was collected from the comment sections of various Shark Tank YouTube videos. We scraped data from popular Shark Tank Youtube Channels. "Shark Tank US," "Shark Tank Global," "Shark Tank India," "Dragons' Den," and "Shark Tank Australia" </p>
            <p><strong>Collection Method:</strong> Instant Data Scraper</p>
            <p><strong>Dataset Size:</strong> Our dataset size is 10,485 total comments split across 33 episodes of 4 different shows.</p>
            <p><strong>Ethical Considerations:</strong> For this project, we protected privacy by only using publicly available comments and reviews that people chose to post online, and we never collected usernames or anything that could identify someone. We kept the data focused on the text itself. Ethically, we made sure not to misrepresent what people were saying, stayed aware of our own biases, and used close reading to avoid misunderstanding things like sarcasm or tone</p>

            <h3>Analysis Methods</h3>
            <p><strong>Tools:</strong> Python (pandas, VADER, Gensim)</p>
            <ul>
                <li><strong>Term Frequency Analysis:</strong> We used term frequency to find the most used words when people were commenting and then compare it between the different shows, as well as to use in tandem with sentiment analysis.</li>
                <li><strong>Sentiment Analysis (VADER):</strong> We looked for sentiment patterns by tracking each shark‚Äôs name and seeing whether the comments connected to that name were positive, negative, or neutral. This helped us see which sharks were talked about the most and what kind of emotions viewers associated with them. We also compared these sentiment scores across different seasons to see how each shark‚Äôs perception changed over time. This let us look for trends, like whether a shark became more liked, more criticized, or stayed consistent as the show and its audience grew.</li>
                <li><strong>Topic Modeling (Gensim LDA):</strong> We trained our LDA Modeler with 3 topics because the comments were shorted than the modeling program would work well with, so more topics tended to just lead to words showing up in multiple topics. Topic Modeling was less effective, but in some cases showed how people would talk about the investors in the shows. Investors names would show up in a topic‚Äôs word bank, and we saw through checking individual comments with topic and the investors name, we could see a lot of the language used to talk about that investor</li>
            </ul>
        </section>

        <section id="analysis">
            <h2>Results & Analysis</h2>
            <!-- TODO: Add visualizations and code snippets -->

            <h3>Sentiment Analysis Results</h3>
            <p>Using VADER sentiment analysis, I examined [describe what you analyzed]...</p>

            <!-- Single visualization with caption -->
            <figure class="viz-container">
                <img src="images/SentDistPie.png"
                     alt="Sentiment distribution pie chart">
                <figcaption>Figure 1: Distribution of sentiment scores across dataset</figcaption>
            </figure>

            <h3>Code Example</h3>
            <p>Here's how I implemented the sentiment analysis using <code>vaderSentiment</code>:</p>

<figure class="viz-container">
                <img src="images/Code Snippet.jpeg"
                     alt="Code Snippet">


            <h3>Topic Modeling Results</h3>
            <p>Using Gensim's LDA implementation, I identified [number] major topics...</p>

            <!-- Multiple visualizations in a grid -->
            <div class="viz-grid">
                <figure class="viz-container">
                    <img src="images/Top20Freq1.png"
                         alt="Visualization of topic clusters from LDA analysis">
                    <figcaption>Figure 2: Topic clusters from LDA analysis</figcaption>
                </figure>

                <figure class="viz-container">
                    <img src="images/word-cloud.png"
                         alt="Word cloud showing most frequent terms">
                    <figcaption>Figure 3: Most common terms in the corpus</figcaption>
                </figure>
            </div>
        </section>

        <section id="findings">
            <h2>Key Findings</h2>
            <!-- TODO: Present your main discoveries -->

            <p>The computational analysis revealed three major insights:</p>

            <ol>
                <li><strong>Finding 1:</strong> [Describe your first key discovery]</li>
                <li><strong>Finding 2:</strong> [Describe your second key discovery]</li>
                <li><strong>Finding 3:</strong> [Describe your third key discovery]</li>
            </ol>

            <h3>Detailed Results</h3>
            <p>Breaking down the sentiment distribution:</p>

            <table class="results-table">
                <thead>
                    <tr>
                        <th>Sentiment Category</th>
                        <th>Count</th>
                        <th>Percentage</th>
                        <th>Avg. Compound Score</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="highlight">
                        <td>Positive</td>
                        <td>XXX</td>
                        <td>XX%</td>
                        <td>0.XX</td>
                    </tr>
                    <tr>
                        <td>Neutral</td>
                        <td>XXX</td>
                        <td>XX%</td>
                        <td>0.XX</td>
                    </tr>
                    <tr>
                        <td>Negative</td>
                        <td>XXX</td>
                        <td>XX%</td>
                        <td>-0.XX</td>
                    </tr>
                </tbody>
            </table>

            <h3>What Surprised Me</h3>
            <p>I initially predicted that [your assumption], but the data revealed [what actually happened]. This challenged my understanding because...</p>
        </section>

        <section id="reflection">
            <h2>Critical Reflection</h2>
            <!-- TODO: Connect to course frameworks -->

            <p>This project demonstrates what happens when coding meets culture by revealing insights that neither computational analysis nor traditional close reading could discover alone.</p>

            <h3>Integration of Methods</h3>
            <p><strong>What computational methods revealed:</strong> [Describe patterns only visible through large-scale analysis]</p>
            <p><strong>What close reading added:</strong> [Describe how interpretive work enriched the computational findings]</p>

            <div class="framework-callout">
                <h3>üìê Classification Logic</h3>
                <p>This project connects to <strong>Classification Logic</strong> by revealing how algorithmic categorization shapes our understanding of [your topic]. [Explain the connection...]</p>

                <p><em>Critical question:</em> What nuances are lost when we reduce complex cultural expressions to computational categories?</p>
            </div>

            <div class="framework-callout">
                <h3>ü§ñ AI Agency</h3>
                <p>The use of topic modeling and sentiment analysis demonstrates <strong>AI Agency</strong> concerns. While the algorithms appear to "discover" meaning, the interpretation and framing of results remains entirely human. [Explain further...]</p>
            </div>

            <h3>Limitations & Future Directions</h3>
            <p><strong>What I would do differently:</strong> [Reflect on your process]</p>
            <p><strong>Questions that remain:</strong> [What would you investigate with more time?]</p>
            <p><strong>Confidence in conclusions:</strong> [How certain are you about your findings? What caveats should readers consider?]</p>
        </section>
    </main>

    <footer>
        <p>üìä <strong>Project Materials:</strong>
            <a href="https://github.com/yourusername/project-name">View Google Colab Notebooks & Data on GitHub</a>
        </p>
        <p>&copy; 2025 Your Name | WRIT 20833: Introduction to Coding in the Humanities</p>
    </footer>
</body>
</html>
