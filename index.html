<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Digital humanities research project exploring [your topic]">
    <meta name="author" content="Your Name">
    <title>Your Project Title | WRIT 20833</title>
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <nav>
        <ul>
            <li><a href="#question">Research Question</a></li>
            <li><a href="#data">Data & Methods</a></li>
            <li><a href="#analysis">Results & Analysis</a></li>
            <li><a href="#findings">Findings</a></li>
            <li><a href="#reflection">Reflection</a></li>
        </ul>
    </nav>

    <header>
        <h1>Shark Tank Around The World!</h1>
        <p>Mia Eberle, Jacob Fenn, Zane Lemaster | WRIT 20833 | Fall 2025</p>
    </header>

    <main>
        <section id="question">
            <h2>Research Question</h2>
            <How has the perception of the sharks on Shark Tank changed over time, along with the general perception of the entire show, as cultures and the world have evolved?
>
            <p>How has the perception of the sharks on Shark Tank changed over time, along with the general perception of the entire show, as cultures and the world have evolved?</p>

            <p><strong>Background:</strong> This research question matters because Shark Tank has been around for so long that people‚Äôs opinions about the show and the sharks have definitely changed, especially as the internet has grown. The sharks are celebrities now, and the way people talk about them online can shape how the show is viewed as a whole. Since online culture has become louder and more opinionated over the years, we wanted to see if those shifts actually show up in the comments and reviews or if the reality is different from what we assume.
We were motivated to study this because earlier in the class we saw how our expectations about online negativity didn‚Äôt always match what the data showed. That made us curious about what might happen if we looked at something bigger and more well-known, like Shark Tank, over a long period of time. By comparing comments from older seasons to newer ones, we want to understand how people‚Äôs reactions have changed and why. Overall, this question matters because it helps us see how TV, internet culture, and public opinion all influence each other, and how using both data tools and human interpretation can reveal things we wouldn‚Äôt notice on our own.</p>
        </section>

        <section id="data">
            <h2>Data & Methods</h2>

            <h3>Dataset</h3>
           
            <p><strong>Data Source:</strong> Our data was collected from the comment sections of various Shark Tank YouTube videos. We scraped data from popular Shark Tank Youtube Channels. "Shark Tank US," "Shark Tank Global," "Shark Tank India," "Dragons' Den," and "Shark Tank Australia" </p>
            <p><strong>Collection Method:</strong> Instant Data Scraper</p>
            <p><strong>Dataset Size:</strong> Our dataset size is 10,485 total comments split across 33 episodes of 4 different shows.</p>
            <p><strong>Ethical Considerations:</strong> For this project, we protected privacy by only using publicly available comments and reviews that people chose to post online, and we never collected usernames or anything that could identify someone. We kept the data focused on the text itself. Ethically, we made sure not to misrepresent what people were saying, stayed aware of our own biases, and used close reading to avoid misunderstanding things like sarcasm or tone</p>

            <h3>Analysis Methods</h3>
            <p><strong>Tools:</strong> Python (pandas, VADER, Gensim)</p>
            <ul>
                <li><strong>Term Frequency Analysis:</strong> We used term frequency to find the most used words when people were commenting and then compare it between the different shows, as well as to use in tandem with sentiment analysis.</li>
                <li><strong>Sentiment Analysis (VADER):</strong> We looked for sentiment patterns by tracking each shark‚Äôs name and seeing whether the comments connected to that name were positive, negative, or neutral. This helped us see which sharks were talked about the most and what kind of emotions viewers associated with them. We also compared these sentiment scores across different seasons to see how each shark‚Äôs perception changed over time. This let us look for trends, like whether a shark became more liked, more criticized, or stayed consistent as the show and its audience grew.</li>
                <li><strong>Topic Modeling (Gensim LDA):</strong> We trained our LDA Modeler with 3 topics because the comments were shorted than the modeling program would work well with, so more topics tended to just lead to words showing up in multiple topics. Topic Modeling was less effective, but in some cases showed how people would talk about the investors in the shows. Investors names would show up in a topic‚Äôs word bank, and we saw through checking individual comments with topic and the investors name, we could see a lot of the language used to talk about that investor</li>
            </ul>
        </section>

        <section id="analysis">
            <h2>Results & Analysis</h2>
          

            <h3>Sentiment Analysis Results</h3>
            <p>Using VADER sentiment analysis, we examined the overall sentiment of the comments to see how many were positive, neutral, or negative and what the general tone of the audience looked like across the shows.</p>

       
            <figure class="viz-container">
                <img src="images/SentDistPie.png"
                     alt="Sentiment distribution pie chart">
                <figcaption>Figure 1: Distribution of sentiment scores across dataset</figcaption>
            </figure>

            <h3>Code Example</h3>
            <p>Here's how we implemented the sentiment analysis using <code>vaderSentiment</code>:</p>

<figure class="viz-container">
                <img src="images/Code Snippet.jpeg"
                     alt="Code Snippet">


            <h3>Topic Modeling Results</h3>
            <p>Using Gensim's LDA implementation, we identified 3 major topics per show...</p>

           
            <div class="viz-grid">
                <figure class="viz-container">
                    <img src="images/Top20Freq1.png"
                         alt="Visualization of topic clusters from LDA analysis">
                    <figcaption>Figure 2: Most common terms in the data scraped</figcaption>
                </figure>

                <figure class="viz-container">
                    <img src="images/TopicModeling.png"
                         alt="Display of 3 topics">
                    <figcaption>Figure 3: Topic clusters from LDA analysis</figcaption>
                </figure>
            </div>
        </section>

        <section id="findings">
            <h2>Key Findings</h2>
           

            <p>The computational analysis revealed three major insights:</p>

            <ol>
                <li><strong>Finding 1:</strong> One of the first things we noticed comparing the data country to country is the overall sentiment score distributions were relatively similar, and contrary to our hypothesis. No country favored a single sentiment by more than 25%, and for all of them negative reviews were the lowest scoring. In our project planning, we had hypothesized that the comments would be mainly negative, as people are more than happy to criticize behind the screen of the internet, but by and large people were making neutral or kind comments.</li>
                <li><strong>Finding 2:</strong> Naturally, the shows we‚Äôre seeing are all based on the same premise and are bound to use similar words, so when doing Term Frequency we expected to find much of the same language between the shows. We did find similar language with ‚Äúbusiness‚Äù, ‚Äúsharks‚Äù, ‚Äúproduct‚Äù, but we also saw the names of the investors on each show come up, which would make sense as they‚Äôre the common denominator between episodes as well as personalities to keep you attached to the show. This showed us that when commenting, people tended to talk about the investors and their decisions as often or more than they talked the actual products.</li>
                <li><strong>Finding 3:</strong> Looking at the term frequency data most used in negative reviews vs positive reviews, we thought that may give us insight into what people are complimenting or criticizing in their comments. Looking at the visualizations, we see that even as the names of the investors may come up more in total for the positive comments, that‚Äôs because there are more positive comments total. we see that negative comments are mentioning investors names more per comment than positive reviews. This would leave us to believe that there‚Äôs more of a likelyhood that the negative comments are critiquing the investors for their decisions, rather than focusing on the product presented or the presenter.</li>
            </ol>

            <h3>Detailed Results</h3>
            <p>Breaking down the sentiment distribution:</p>

            <table class="results-table">
                <thead>
                    <tr>
                        <th>Sentiment Category</th>
                        <th>Count</th>
                        <th>Percentage</th>
                        <th>Avg. Compound Score</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="highlight">
                        <td>Positive</td>
                        <td>203</td>
                        <td>33.9%</td>
                        <td>0.52</td>
                    </tr>
                    <tr>
                        <td>Neutral</td>
                        <td>330</td>
                        <td>55.2%</td>
                        <td>0.02</td>
                    </tr>
                    <tr>
                        <td>Negative</td>
                        <td>65</td>
                        <td>10.9%</td>
                        <td>-0.47</td>
                    </tr>
                </tbody>
            </table>

            <h3>What Surprised Us</h3>
            <p>We initially thought the comments would be mostly negative and that people online would mainly talk about the products and the entrepreneurs. But the data showed almost the opposite. Most comments were neutral or positive, and the sharks were mentioned way more than anything else. Even the negative comments focused on the sharks and their decisions instead of the pitches. We were also surprised that the sentiment stayed almost the same across different countries, even though the shows aren‚Äôt identical. This challenged what we expected because it showed that online audiences aren‚Äôt always as harsh as we assume, and that the sharks really are the center of the conversation no matter where the show is airing.</p>
        </section>
 <section id="Essay">
            <h2>Research Essay</h2>

        <section id="reflection">
            <h2>Critical Reflection</h2>
            <p> Shark Tank Around the World
Our project focuses on understanding how viewers talk about the sharks on Shark Tank and how that perception has changed over time and across different versions of the show. Shark Tank has been around for many years now, and because of that, the sharks have become major public figures whose personalities and decisions get talked about constantly. Online culture has also changed a lot during this time, becoming more opinionated and more global. These changes made us wonder whether people‚Äôs reactions to the show have shifted in the same way. We originally expected the comments to be mostly negative, because online spaces often feel harsh or critical, but our earlier deliverables in this class showed us that assumptions about online negativity can be totally wrong. That motivated us to explore the topic on a much larger scale by studying thousands of comments across different countries. Our goal was to see what people actually say about the sharks and whether that changes over time, instead of assuming we already knew the answer.
The main research question guiding our project was how the perception of the sharks on Shark Tank has changed over time, and how the general perception of the entire show has held up and shifted with ever-changing cultures. This question matters because Shark Tank isn‚Äôt just entertainment, but rather it influences how people think about entrepreneurship and business values, and the sharks themselves have become symbols of those ideas. Understanding how viewers respond to them helps us understand how public attitudes toward business, success, and even risk-taking change in different cultural moments.
To answer our question, we built a dataset of over 10,000 YouTube comments taken from 33 episodes across several international versions of the show, including Shark Tank US, Shark Tank Global, Shark Tank India, Shark Tank Australia, and Dragons‚Äô Den (United Kingdom). We scraped the comments of each one of these episodes, hoping to find a correlation between names, topics, and various patterns. We made sure to keep our focus strictly on the text, and kept an open mind for anything we might find upon data analysis. It was important to us that we handled the data ethically, since real people wrote these comments. We made sure not to misrepresent anyone‚Äôs words and this is where we were able to utilize close reading to double-check when the computer might have misunderstood sarcasm, jokes, or tone.
Once our dataset was cleaned, we analyzed it using Python. We used pandas for organizing the comments, VADER for sentiment analysis, and Gensim‚Äôs LDA for topic modeling. We also used term frequency analysis to see which words appeared the most often, especially names of sharks and recurring vocabulary related to business or pitching. Term frequency helped us notice right away which people or ideas dominated the conversation. Sentiment analysis helped us measure whether comments were mostly positive, negative, or neutral. This allowed us to track patterns in how viewers emotionally responded to each investor. Topic modeling was meant to reveal larger themes people were discussing, although it ended up being tricky because YouTube comments are usually short. Still, it gave us a general sense of what topics tended to form in different versions of the show.
While we were working with these tools, we also realized how much small decisions on our end shaped the output. For example, choosing to include or exclude certain words from our analysis could completely change what the algorithm focused on. Even deciding how many topics to generate in the LDA model changed the story the data seemed to tell. This made us more aware that this type of data analysis isn‚Äôt just a neutral, automatic process. There‚Äôs a lot of human choice built into it, and those choices influence what patterns appear and which ones stay hidden. Technology and coding resources are fantastic, and they all typically do their job very well, but the human mind behind it is just as important. Understanding this helped us read the results more carefully instead of assuming the computer was always right without running it by a pair of human eyes.
Putting all these methods together helped us see different layers of meaning in the comments. The computational tools gave us patterns we couldn‚Äôt have found by hand, while close reading helped us make sure the data wasn‚Äôt misleading. For example, a comment saying ‚ÄúOh great job, Kevin, totally fair‚Ä¶‚Äù could be sarcastic, and a computer might label it as positive. Close reading would help to avoid that. This combination of methods made our analysis much stronger, because no single tool could tell the whole story on its own.
When we looked at the sentiment analysis results, we were surprised. Instead of seeing mostly negative comments like we originally expected, we found that over half of the comments were neutral and about a third were positive. Only a small percentage were negative. This pattern was consistent across different countries, which showed us that online audiences around the world respond to these shows in surprisingly similar ways. We expected bigger differences between countries, but the sentiment distributions were almost identical. This challenged our assumptions about how harsh people online really are and made us realize that viewers tend to be more supportive or neutral than critical.
Term frequency revealed another major insight. Across all shows, shark names came up constantly. In fact, people talked about the sharks even more than the entrepreneurs or products being pitched. This suggests that the investors are the true center of the show in the eyes of the audience. Even when a pitch was interesting, many commenters focused more on a shark‚Äôs reaction, advice, or personality. This held true in positive comments, neutral ones, and especially negative ones. When viewers did criticize something, it was almost always a shark‚Äôs decision or attitude rather than the entrepreneur or product. That showed us that negativity, when it appears, is usually directed at the investors because viewers feel strongly about the choices they make.
Topic modeling gave us a smaller amount of insight compared to the other methods, mainly because the comments were so short that the model couldn‚Äôt form strong topics. However, it did still reveal that many comments clustered around investor behavior, specific shark personalities, and reactions to decisions made during the pitches. Even though the model didn‚Äôt create perfect topics, it still supported the idea that the sharks dominate the discussion.
Overall, these results taught us that the online conversation around Shark Tank is a lot more positive and shark-focused than we expected. We thought people would mostly criticize the entrepreneurs or the products, but instead the investors took center stage in almost every category of analysis. We were also surprised that the tone of comments didn‚Äôt shift dramatically across countries, even though cultural norms and business values differ. The clear language barrier in certain countries did skew our data, but overall the findings remained similar. This made us think more about how global media shapes conversations and creates similar audience behavior across cultures.
Reflecting on the process, this project showed us why computational tools and humanities skills are both necessary for understanding cultural data. The computational tools let us process thousands of comments and find patterns that our eyes alone would never catch. But the humanities side gave us the ability to understand tone, meaning, and nuance that algorithms simply can‚Äôt understand. Together, they created a fuller picture of how people talk about Shark Tank and what those comments actually mean. It also made us more aware of the role of algorithmic classification. The tools we used, like VADER and LDA, categorized the comments in ways that shaped our understanding of the results. For example, if VADER labeled a comment as neutral, we tended to treat it that way even though a human reader might see subtle positivity or frustration. This raised an important question about what gets lost when people‚Äôs complex expressions are reduced to a few categories. AI can‚Äôt understand humor, emotional nuance, or cultural context the way humans can, so our interpretation still mattered a lot.
There were definitely limitations to our project. Our dataset definitely was not perfectly balanced across seasons or countries (because these factors varied), and sentiment tools struggled with sarcasm. Topic modeling also didn‚Äôt work as well as we hoped because short comments don‚Äôt give the model much to analyze. If we had more time, we would want to gather a larger and more evenly distributed dataset, possibly from one season at a time to make time-based comparisons more accurate. We would also want to use more advanced language tools that understand sarcasm better, and maybe compare YouTube comments to platforms like Reddit or TikTok to see if audiences behave differently there.
Even with these limitations, we feel confident in the big conclusions we found. People around the world talk about Shark Tank in surprisingly similar ways, the overall sentiment is more neutral and positive than expected, and the sharks are consistently the main focus of conversation. These findings show that while online culture can seem absolutely crazy, chaotic, and rude, people‚Äôs reactions to popular shows like Shark Tank tend to follow clear patterns that become visible when key computational methods are utilized and studied.
</p>

            <p>This project shows what happens when coding and culture work together. By combining both, we were able to understand Shark Tank comments in a way that neither data analysis nor close reading could fully explain on their own. Using code helped us see the big patterns across thousands of comments, while close reading helped us understand the meaning behind those patterns and avoid misinterpreting things like sarcasm or jokes.
.</p>

            <h3>Integration of Methods</h3>
            <p><strong>What computational methods revealed:</strong> The computational tools showed us patterns we never could have found by hand. We saw how often each shark was mentioned, how sentiment changed across different countries, and which words appeared the most in positive vs. negative comments. We also learned that overall sentiment was way more neutral and positive than we expected. These larger trends only became obvious because we used Python, VADER, and LDA to analyze hundreds upon hundreds of comments at once.
</p>
            <p><strong>What close reading added:</strong> Close reading helped us to more deeply understand why people were talking the way they were. By actually reading individual comments connected to topics and sentiment scores, we could tell when people were joking, arguing, or being sarcastic, and overall many things the computer can‚Äôt always pick up. It also helped us understand why certain sharks got more criticism or praise, and how specific comments reflected people‚Äôs attitudes beyond just the numbers. Close reading basically filled in all the details that the computational tools couldn‚Äôt explain on their own.
</p>

            <div class="framework-callout">
                <h3>üìê Classification Logic</h3>
                <p>This project connects to <strong>Classification Logic</strong> by revealing how algorithmic categorization shapes our understanding of Shark Tank comments. This project connects to Classification Logic because all of our results depended on how the algorithms sorted comments into categories like positive, negative, neutral, or certain topics. The way VADER and LDA classified things shaped the patterns we ended up seeing. For example, if the algorithm said a comment was ‚Äúneutral,‚Äù we tended to treat it as calm or informational, even if a human might read it differently. So the computational categories basically guided what we thought the main story of the data was. This shows how much our understanding depends on the labels the system creates, not just on the raw text itself.
</p>

                <p><em>Critical question: What nuances are lost when we reduce complex cultural expressions to computational categories?</em>
                    A lot of personal tone, humor, sarcasm, and emotion gets flattened when everything is forced into just a few buckets. Real people comment for all kinds of reasons, but computational tools can only sort them into simple labels. That means some meaning might get lost, or comments might get interpreted in a way that doesn‚Äôt fully match what the person meant</p>
            </div>

            <div class="framework-callout">
                <h3>ü§ñ AI Agency</h3>
                <p>The use of topic modeling and sentiment analysis demonstrates <strong>AI Agency</strong> concerns. While the algorithms appear to "discover" meaning, the interpretation and framing of results remains entirely human. The use of topic modeling and sentiment analysis brings up AI Agency issues because it can look like the computer is ‚Äúfinding‚Äù meaning on its own. But really, the algorithms aren‚Äôt understanding anything, rather, they‚Äôre just following rules we gave them. We had to choose how many topics to use, which words to include, and how to interpret the sentiment scores afterward. The computer only gave us patterns, and we had to decide what those patterns actually meant. So even though the AI tools seem powerful, we really did find that the human interpretation is still the part that decides the real story.</p>
            </div>

            <h3>Limitations & Future Directions</h3>
            <p><strong>What we would do differently:</strong> If we could redo the project, we would collect a more balanced dataset with comments from specific seasons or years to get a clearer sense of how perception changed over time. we‚Äôd also try using more advanced sentiment tools that handle sarcasm better, since that was something VADER struggled with. Another thing we would change is how we approached topic modeling. We only did 3 topics per show, but since each show was analyzed separately, there weren‚Äôt 3 big overarching topics across the whole dataset. Plus, the comments were so short that the model didn‚Äôt have much to work with, so the topics didn‚Äôt end up being very strong or useful. In the future, we‚Äôd probably either combine all shows into one model, use more comments overall, or try a method that works better with short text.
</p>
            <p><strong>Questions that remain:</strong> With more time, we'd want to look even more deeply at each shark individually and track how their public perception shifts across different eras of the show and generate super specific metrics to track that. We would also want to compare comments across social media platforms, like TikTok or Reddit, to see if fans talk differently there.</p>
            <p><strong>Confidence in conclusions:</strong> We feel fairly confident about the broad trends we found, like the high amount of neutral and positive sentiment and the focus on the sharks over the products. But there are caveats such as our dataset wasn‚Äôt perfectly even across countries and seasons, and it was also noted that the tools can‚Äôt always read tone correctly. Topic modeling (as noted earlier) also did not work as well with much of this data as initially anticipated. So the conclusions give a strong general picture, but they shouldn‚Äôt be taken as the full story of how every viewer feels.</p>
        </section>
    </main>

    <footer>
        <p>üìä <strong>Project Materials:</strong>
            <a href="https://github.com/jacobfenn/shark-tank">View Google Colab Notebooks & Data on GitHub</a>
        </p>
        <p>&copy; 2025 Mia Eberle, Jacob Fenn, Zane Lemaster | WRIT 20833: Introduction to Coding in the Humanities</p>
    </footer>
</body>
</html>
