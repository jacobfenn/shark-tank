<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Digital humanities research project exploring [your topic]">
    <meta name="author" content="Your Name">
    <title>Your Project Title | WRIT 20833</title>
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <nav>
        <ul>
            <li><a href="#question">Research Question</a></li>
            <li><a href="#data">Data & Methods</a></li>
            <li><a href="#analysis">Results & Analysis</a></li>
            <li><a href="#findings">Findings</a></li>
            <li><a href="#reflection">Reflection</a></li>
        </ul>
    </nav>

    <header>
        <h1>Shark Tank Around The World!</h1>
        <p>Mia Eberle, Jacob Fenn, Zane Lemaster | WRIT 20833 | Fall 2025</p>
    </header>

    <main>
        <section id="question">
            <h2>Research Question</h2>
            <How has the perception of the sharks on Shark Tank changed over time, along with the general perception of the entire show, as cultures and the world have evolved?
>
            <p>How has the perception of the sharks on Shark Tank changed over time, along with the general perception of the entire show, as cultures and the world have evolved?</p>

            <p><strong>Background:</strong> This research question matters because Shark Tank has been around for so long that people‚Äôs opinions about the show and the sharks have definitely changed, especially as the internet has grown. The sharks are celebrities now, and the way people talk about them online can shape how the show is viewed as a whole. Since online culture has become louder and more opinionated over the years, we wanted to see if those shifts actually show up in the comments and reviews or if the reality is different from what we assume.
We were motivated to study this because earlier in the class we saw how our expectations about online negativity didn‚Äôt always match what the data showed. That made us curious about what might happen if we looked at something bigger and more well-known, like Shark Tank, over a long period of time. By comparing comments from older seasons to newer ones, we want to understand how people‚Äôs reactions have changed and why. Overall, this question matters because it helps us see how TV, internet culture, and public opinion all influence each other, and how using both data tools and human interpretation can reveal things we wouldn‚Äôt notice on our own.</p>
        </section>

        <section id="data">
            <h2>Data & Methods</h2>

            <h3>Dataset</h3>
            <!-- TODO: Describe your data collection and methodology -->
            <p><strong>Data Source:</strong> Our data was collected from the comment sections of various Shark Tank YouTube videos. We scraped data from popular Shark Tank Youtube Channels. "Shark Tank US," "Shark Tank Global," "Shark Tank India," "Dragons' Den," and "Shark Tank Australia" </p>
            <p><strong>Collection Method:</strong> Instant Data Scraper</p>
            <p><strong>Dataset Size:</strong> Our dataset size is 10,485 total comments split across 33 episodes of 4 different shows.</p>
            <p><strong>Ethical Considerations:</strong> For this project, we protected privacy by only using publicly available comments and reviews that people chose to post online, and we never collected usernames or anything that could identify someone. We kept the data focused on the text itself. Ethically, we made sure not to misrepresent what people were saying, stayed aware of our own biases, and used close reading to avoid misunderstanding things like sarcasm or tone</p>

            <h3>Analysis Methods</h3>
            <p><strong>Tools:</strong> Python (pandas, VADER, Gensim)</p>
            <ul>
                <li><strong>Term Frequency Analysis:</strong> We used term frequency to find the most used words when people were commenting and then compare it between the different shows, as well as to use in tandem with sentiment analysis.</li>
                <li><strong>Sentiment Analysis (VADER):</strong> We looked for sentiment patterns by tracking each shark‚Äôs name and seeing whether the comments connected to that name were positive, negative, or neutral. This helped us see which sharks were talked about the most and what kind of emotions viewers associated with them. We also compared these sentiment scores across different seasons to see how each shark‚Äôs perception changed over time. This let us look for trends, like whether a shark became more liked, more criticized, or stayed consistent as the show and its audience grew.</li>
                <li><strong>Topic Modeling (Gensim LDA):</strong> We trained our LDA Modeler with 3 topics because the comments were shorted than the modeling program would work well with, so more topics tended to just lead to words showing up in multiple topics. Topic Modeling was less effective, but in some cases showed how people would talk about the investors in the shows. Investors names would show up in a topic‚Äôs word bank, and we saw through checking individual comments with topic and the investors name, we could see a lot of the language used to talk about that investor</li>
            </ul>
        </section>

        <section id="analysis">
            <h2>Results & Analysis</h2>
            <!-- TODO: Add visualizations and code snippets -->

            <h3>Sentiment Analysis Results</h3>
            <p>Using VADER sentiment analysis, we examined [describe what you analyzed]...</p>

            <!-- Single visualization with caption -->
            <figure class="viz-container">
                <img src="images/SentDistPie.png"
                     alt="Sentiment distribution pie chart">
                <figcaption>Figure 1: Distribution of sentiment scores across dataset</figcaption>
            </figure>

            <h3>Code Example</h3>
            <p>Here's how we implemented the sentiment analysis using <code>vaderSentiment</code>:</p>

<figure class="viz-container">
                <img src="images/Code Snippet.jpeg"
                     alt="Code Snippet">


            <h3>Topic Modeling Results</h3>
            <p>Using Gensim's LDA implementation, we identified 3 major topics per show...</p>

            <!-- Multiple visualizations in a grid -->
            <div class="viz-grid">
                <figure class="viz-container">
                    <img src="images/Top20Freq1.png"
                         alt="Visualization of topic clusters from LDA analysis">
                    <figcaption>Figure 2: Most common terms in the data scraped</figcaption>
                </figure>

                <figure class="viz-container">
                    <img src="images/TopicModeling.png"
                         alt="Display of 3 topics">
                    <figcaption>Figure 3: Topic clusters from LDA analysis</figcaption>
                </figure>
            </div>
        </section>

        <section id="findings">
            <h2>Key Findings</h2>
            <!-- TODO: Present your main discoveries -->

            <p>The computational analysis revealed three major insights:</p>

            <ol>
                <li><strong>Finding 1:</strong> One of the first things we noticed comparing the data country to country is the overall sentiment score distribution were relatively similar, and contrary to our hypothesis. No country favored a single sentiment by more than 25%, and for all of them negative reviews were the lowest scoring. In our project planning, we had hypothesized that the comments would be mainly negative, as people are more than happy to criticize behind the screen of the internet, but by and large people were making neutral or kind comments.</li>
                <li><strong>Finding 2:</strong> Naturally, the shows we‚Äôre seeing are all based on the same premise and are bound to use similar words, so when doing Term Frequency we expected to find much of the same language between the shows. We did find similar language with ‚Äúbusiness‚Äù, ‚Äúsharks‚Äù, ‚Äúproduct‚Äù, but we also saw the names of the investors on each show come up, which would make sense as they‚Äôre the common denominator between episodes as well as personalities to keep you attached to the show. This showed us that when commenting, people tended to talk about the investors and their decisions as often or more than they talked the actual products.</li>
                <li><strong>Finding 3:</strong> Looking at the term frequency data most used in negative reviews vs positive reviews, we thought that may give us insight into what people are complimenting or criticizing in their comments. Looking at the visualizations, we see that even as the names of the investors may come up more in total for the positive comments, that‚Äôs because there are more positive comments total. we see that negative comments are mentioning investors names more per comment than positive reviews. This would leave us to believe that there‚Äôs more of a likelyhood that the negative comments are critiquing the investors for their decisions, rather than focusing on the product presented or the presenter.</li>
            </ol>

            <h3>Detailed Results</h3>
            <p>Breaking down the sentiment distribution:</p>

            <table class="results-table">
                <thead>
                    <tr>
                        <th>Sentiment Category</th>
                        <th>Count</th>
                        <th>Percentage</th>
                        <th>Avg. Compound Score</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="highlight">
                        <td>Positive</td>
                        <td>203</td>
                        <td>33.9%</td>
                        <td>0.52</td>
                    </tr>
                    <tr>
                        <td>Neutral</td>
                        <td>330</td>
                        <td>55.2%</td>
                        <td>0.02</td>
                    </tr>
                    <tr>
                        <td>Negative</td>
                        <td>65</td>
                        <td>10.9%</td>
                        <td>-0.47</td>
                    </tr>
                </tbody>
            </table>

            <h3>What Surprised Me</h3>
            <p>I initially predicted that [your assumption], but the data revealed [what actually happened]. This challenged my understanding because...</p>
        </section>

        <section id="reflection">
            <h2>Critical Reflection</h2>
            <!-- TODO: Connect to course frameworks -->

            <p>This project demonstrates what happens when coding meets culture by revealing insights that neither computational analysis nor traditional close reading could discover alone.</p>

            <h3>Integration of Methods</h3>
            <p><strong>What computational methods revealed:</strong> [Describe patterns only visible through large-scale analysis]</p>
            <p><strong>What close reading added:</strong> [Describe how interpretive work enriched the computational findings]</p>

            <div class="framework-callout">
                <h3>üìê Classification Logic</h3>
                <p>This project connects to <strong>Classification Logic</strong> by revealing how algorithmic categorization shapes our understanding of [your topic]. [Explain the connection...]</p>

                <p><em>Critical question:</em> What nuances are lost when we reduce complex cultural expressions to computational categories?</p>
            </div>

            <div class="framework-callout">
                <h3>ü§ñ AI Agency</h3>
                <p>The use of topic modeling and sentiment analysis demonstrates <strong>AI Agency</strong> concerns. While the algorithms appear to "discover" meaning, the interpretation and framing of results remains entirely human. [Explain further...]</p>
            </div>

            <h3>Limitations & Future Directions</h3>
            <p><strong>What I would do differently:</strong> [Reflect on your process]</p>
            <p><strong>Questions that remain:</strong> [What would you investigate with more time?]</p>
            <p><strong>Confidence in conclusions:</strong> [How certain are you about your findings? What caveats should readers consider?]</p>
        </section>
    </main>

    <footer>
        <p>üìä <strong>Project Materials:</strong>
            <a href="https://github.com/yourusername/project-name">View Google Colab Notebooks & Data on GitHub</a>
        </p>
        <p>&copy; 2025 Your Name | WRIT 20833: Introduction to Coding in the Humanities</p>
    </footer>
</body>
</html>
