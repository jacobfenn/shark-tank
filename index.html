<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Digital humanities research project exploring [your topic]">
    <meta name="author" content="Your Name">
    <title>Your Project Title | WRIT 20833</title>
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <nav>
        <ul>
            <li><a href="#question">Research Question</a></li>
            <li><a href="#data">Data & Methods</a></li>
            <li><a href="#analysis">Results & Analysis</a></li>
            <li><a href="#findings">Findings</a></li>
            <li><a href="#reflection">Reflection</a></li>
        </ul>
    </nav>

    <header>
        <h1>Shark Tank Around The World!</h1>
        <p>Mia Eberle, Jacob Fenn, Zane Lemaster | WRIT 20833 | Fall 2025</p>
    </header>

    <main>
        <section id="question">
            <h2>Research Question</h2>
            <How has the perception of the sharks on Shark Tank changed over time, along with the general perception of the entire show, as cultures and the world have evolved?
>
            <p>How has the perception of the sharks on Shark Tank changed over time, along with the general perception of the entire show, as cultures and the world have evolved?</p>

            <p><strong>Background:</strong> This research question matters because Shark Tank has been around for so long that people‚Äôs opinions about the show and the sharks have definitely changed, especially as the internet has grown. The sharks are celebrities now, and the way people talk about them online can shape how the show is viewed as a whole. Since online culture has become louder and more opinionated over the years, we wanted to see if those shifts actually show up in the comments and reviews or if the reality is different from what we assume.
We were motivated to study this because earlier in the class we saw how our expectations about online negativity didn‚Äôt always match what the data showed. That made us curious about what might happen if we looked at something bigger and more well-known, like Shark Tank, over a long period of time. By comparing comments from older seasons to newer ones, we want to understand how people‚Äôs reactions have changed and why. Overall, this question matters because it helps us see how TV, internet culture, and public opinion all influence each other, and how using both data tools and human interpretation can reveal things we wouldn‚Äôt notice on our own.</p>
        </section>

        <section id="data">
            <h2>Data & Methods</h2>

            <h3>Dataset</h3>
           
            <p><strong>Data Source:</strong> Our data was collected from the comment sections of various Shark Tank YouTube videos. We scraped data from popular Shark Tank Youtube Channels. "Shark Tank US," "Shark Tank Global," "Shark Tank India," "Dragons' Den," and "Shark Tank Australia" </p>
            <p><strong>Collection Method:</strong> Instant Data Scraper</p>
            <p><strong>Dataset Size:</strong> Our dataset size is 10,485 total comments split across 33 episodes of 4 different shows.</p>
            <p><strong>Ethical Considerations:</strong> For this project, we protected privacy by only using publicly available comments and reviews that people chose to post online, and we never collected usernames or anything that could identify someone. We kept the data focused on the text itself. Ethically, we made sure not to misrepresent what people were saying, stayed aware of our own biases, and used close reading to avoid misunderstanding things like sarcasm or tone</p>

            <h3>Analysis Methods</h3>
            <p><strong>Tools:</strong> Python (pandas, VADER, Gensim)</p>
            <ul>
                <li><strong>Term Frequency Analysis:</strong> We used term frequency to find the most used words when people were commenting and then compare it between the different shows, as well as to use in tandem with sentiment analysis.</li>
                <li><strong>Sentiment Analysis (VADER):</strong> We looked for sentiment patterns by tracking each shark‚Äôs name and seeing whether the comments connected to that name were positive, negative, or neutral. This helped us see which sharks were talked about the most and what kind of emotions viewers associated with them. We also compared these sentiment scores across different seasons to see how each shark‚Äôs perception changed over time. This let us look for trends, like whether a shark became more liked, more criticized, or stayed consistent as the show and its audience grew.</li>
                <li><strong>Topic Modeling (Gensim LDA):</strong> We trained our LDA Modeler with 3 topics because the comments were shorted than the modeling program would work well with, so more topics tended to just lead to words showing up in multiple topics. Topic Modeling was less effective, but in some cases showed how people would talk about the investors in the shows. Investors names would show up in a topic‚Äôs word bank, and we saw through checking individual comments with topic and the investors name, we could see a lot of the language used to talk about that investor</li>
            </ul>
        </section>

        <section id="analysis">
            <h2>Results & Analysis</h2>
          

            <h3>Sentiment Analysis Results</h3>
            <p>Using VADER sentiment analysis, we examined the overall sentiment of the comments to see how many were positive, neutral, or negative and what the general tone of the audience looked like across the shows.</p>

       
            <figure class="viz-container">
                <img src="images/SentDistPie.png"
                     alt="Sentiment distribution pie chart">
                <figcaption>Figure 1: Distribution of sentiment scores across dataset</figcaption>
            </figure>

            <h3>Code Example</h3>
            <p>Here's how we implemented the sentiment analysis using <code>vaderSentiment</code>:</p>

<figure class="viz-container">
                <img src="images/Code Snippet.jpeg"
                     alt="Code Snippet">


            <h3>Topic Modeling Results</h3>
            <p>Using Gensim's LDA implementation, we identified 3 major topics per show...</p>

           
            <div class="viz-grid">
                <figure class="viz-container">
                    <img src="images/Top20Freq1.png"
                         alt="Visualization of topic clusters from LDA analysis">
                    <figcaption>Figure 2: Most common terms in the data scraped</figcaption>
                </figure>

                <figure class="viz-container">
                    <img src="images/TopicModeling.png"
                         alt="Display of 3 topics">
                    <figcaption>Figure 3: Topic clusters from LDA analysis</figcaption>
                </figure>
            </div>
        </section>

        <section id="findings">
            <h2>Key Findings</h2>
           

            <p>The computational analysis revealed three major insights:</p>

            <ol>
                <li><strong>Finding 1:</strong> One of the first things we noticed comparing the data country to country is the overall sentiment score distributions were relatively similar, and contrary to our hypothesis. No country favored a single sentiment by more than 25%, and for all of them negative reviews were the lowest scoring. In our project planning, we had hypothesized that the comments would be mainly negative, as people are more than happy to criticize behind the screen of the internet, but by and large people were making neutral or kind comments.</li>
                <li><strong>Finding 2:</strong> Naturally, the shows we‚Äôre seeing are all based on the same premise and are bound to use similar words, so when doing Term Frequency we expected to find much of the same language between the shows. We did find similar language with ‚Äúbusiness‚Äù, ‚Äúsharks‚Äù, ‚Äúproduct‚Äù, but we also saw the names of the investors on each show come up, which would make sense as they‚Äôre the common denominator between episodes as well as personalities to keep you attached to the show. This showed us that when commenting, people tended to talk about the investors and their decisions as often or more than they talked the actual products.</li>
                <li><strong>Finding 3:</strong> Looking at the term frequency data most used in negative reviews vs positive reviews, we thought that may give us insight into what people are complimenting or criticizing in their comments. Looking at the visualizations, we see that even as the names of the investors may come up more in total for the positive comments, that‚Äôs because there are more positive comments total. we see that negative comments are mentioning investors names more per comment than positive reviews. This would leave us to believe that there‚Äôs more of a likelyhood that the negative comments are critiquing the investors for their decisions, rather than focusing on the product presented or the presenter.</li>
            </ol>

            <h3>Detailed Results</h3>
            <p>Breaking down the sentiment distribution:</p>

            <table class="results-table">
                <thead>
                    <tr>
                        <th>Sentiment Category</th>
                        <th>Count</th>
                        <th>Percentage</th>
                        <th>Avg. Compound Score</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="highlight">
                        <td>Positive</td>
                        <td>203</td>
                        <td>33.9%</td>
                        <td>0.52</td>
                    </tr>
                    <tr>
                        <td>Neutral</td>
                        <td>330</td>
                        <td>55.2%</td>
                        <td>0.02</td>
                    </tr>
                    <tr>
                        <td>Negative</td>
                        <td>65</td>
                        <td>10.9%</td>
                        <td>-0.47</td>
                    </tr>
                </tbody>
            </table>

            <h3>What Surprised Us</h3>
            <p>We initially thought the comments would be mostly negative and that people online would mainly talk about the products and the entrepreneurs. But the data showed almost the opposite. Most comments were neutral or positive, and the sharks were mentioned way more than anything else. Even the negative comments focused on the sharks and their decisions instead of the pitches. We were also surprised that the sentiment stayed almost the same across different countries, even though the shows aren‚Äôt identical. This challenged what we expected because it showed that online audiences aren‚Äôt always as harsh as we assume, and that the sharks really are the center of the conversation no matter where the show is airing.</p>
        </section>

        <section id="reflection">
            <h2>Critical Reflection</h2>
           

            <p>This project shows what happens when coding and culture work together. By combining both, we were able to understand Shark Tank comments in a way that neither data analysis nor close reading could fully explain on their own. Using code helped us see the big patterns across thousands of comments, while close reading helped us understand the meaning behind those patterns and avoid misinterpreting things like sarcasm or jokes.
.</p>

            <h3>Integration of Methods</h3>
            <p><strong>What computational methods revealed:</strong> The computational tools showed us patterns we never could have found by hand. We saw how often each shark was mentioned, how sentiment changed across different countries, and which words appeared the most in positive vs. negative comments. We also learned that overall sentiment was way more neutral and positive than we expected. These larger trends only became obvious because we used Python, VADER, and LDA to analyze hundreds upon hundreds of comments at once.
</p>
            <p><strong>What close reading added:</strong> Close reading helped us to more deeply understand why people were talking the way they were. By actually reading individual comments connected to topics and sentiment scores, we could tell when people were joking, arguing, or being sarcastic, and overall many things the computer can‚Äôt always pick up. It also helped us understand why certain sharks got more criticism or praise, and how specific comments reflected people‚Äôs attitudes beyond just the numbers. Close reading basically filled in all the details that the computational tools couldn‚Äôt explain on their own.
</p>

            <div class="framework-callout">
                <h3>üìê Classification Logic</h3>
                <p>This project connects to <strong>Classification Logic</strong> by revealing how algorithmic categorization shapes our understanding of Shark Tank comments. This project connects to Classification Logic because all of our results depended on how the algorithms sorted comments into categories like positive, negative, neutral, or certain topics. The way VADER and LDA classified things shaped the patterns we ended up seeing. For example, if the algorithm said a comment was ‚Äúneutral,‚Äù we tended to treat it as calm or informational, even if a human might read it differently. So the computational categories basically guided what we thought the main story of the data was. This shows how much our understanding depends on the labels the system creates, not just on the raw text itself.
</p>

                <p><em>Critical question: What nuances are lost when we reduce complex cultural expressions to computational categories?</em>
                    A lot of personal tone, humor, sarcasm, and emotion gets flattened when everything is forced into just a few buckets. Real people comment for all kinds of reasons, but computational tools can only sort them into simple labels. That means some meaning might get lost, or comments might get interpreted in a way that doesn‚Äôt fully match what the person meant</p>
            </div>

            <div class="framework-callout">
                <h3>ü§ñ AI Agency</h3>
                <p>The use of topic modeling and sentiment analysis demonstrates <strong>AI Agency</strong> concerns. While the algorithms appear to "discover" meaning, the interpretation and framing of results remains entirely human. The use of topic modeling and sentiment analysis brings up AI Agency issues because it can look like the computer is ‚Äúfinding‚Äù meaning on its own. But really, the algorithms aren‚Äôt understanding anything, rather, they‚Äôre just following rules we gave them. We had to choose how many topics to use, which words to include, and how to interpret the sentiment scores afterward. The computer only gave us patterns, and we had to decide what those patterns actually meant. So even though the AI tools seem powerful, we really did find that the human interpretation is still the part that decides the real story.</p>
            </div>

            <h3>Limitations & Future Directions</h3>
            <p><strong>What we would do differently:</strong> If we could redo the project, we would collect a more balanced dataset with comments from specific seasons or years to get a clearer sense of how perception changed over time. we‚Äôd also try using more advanced sentiment tools that handle sarcasm better, since that was something VADER struggled with. Another thing we would change is how we approached topic modeling. We only did 3 topics per show, but since each show was analyzed separately, there weren‚Äôt 3 big overarching topics across the whole dataset. Plus, the comments were so short that the model didn‚Äôt have much to work with, so the topics didn‚Äôt end up being very strong or useful. In the future, we‚Äôd probably either combine all shows into one model, use more comments overall, or try a method that works better with short text.
</p>
            <p><strong>Questions that remain:</strong> With more time, we'd want to look even more deeply at each shark individually and track how their public perception shifts across different eras of the show and generate super specific metrics to track that. We would also want to compare comments across social media platforms, like TikTok or Reddit, to see if fans talk differently there.</p>
            <p><strong>Confidence in conclusions:</strong> We feel fairly confident about the broad trends we found, like the high amount of neutral and positive sentiment and the focus on the sharks over the products. But there are caveats such as our dataset wasn‚Äôt perfectly even across countries and seasons, and it was also noted that the tools can‚Äôt always read tone correctly. Topic modeling (as noted earlier) also did not work as well with much of this data as initially anticipated. So the conclusions give a strong general picture, but they shouldn‚Äôt be taken as the full story of how every viewer feels.</p>
        </section>
    </main>

    <footer>
        <p>üìä <strong>Project Materials:</strong>
            <a href="https://github.com/jacobfenn/shark-tank">View Google Colab Notebooks & Data on GitHub</a>
        </p>
        <p>&copy; 2025 Mia Eberle, Jacob Fenn, Zane Lemaster | WRIT 20833: Introduction to Coding in the Humanities</p>
    </footer>
</body>
</html>
